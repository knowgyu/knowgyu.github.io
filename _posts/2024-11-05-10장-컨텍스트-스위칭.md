---
title: "10장 컨텍스트 스위칭"
author: knowgyu
description: " "
date: 2024-10-21 06:55:52 +0900
math: true
categories: [Embedded System, 임베디드 OS 개발 프로젝트]
tags: [Embedded System, RTOS, Firmware, OS]
---

![image.png](/assets/img/OS/OS000.jpg){: .w-25}
<br>

**본 프로젝트는 이만우 저자님의 "임베디드 OS 개발 프로젝트" 교재를 따라 RTOS를 만드는 것을 목표로 합니다.**

<br>

***

본 페이지는 컨텍스트 스위칭에 대해 다룹니다.

> [https://github.com/navilera/Navilos/tree/f02ff6b92d2a356f85c76a60a12ef6ea73fdbd81](https://github.com/navilera/Navilos/tree/f02ff6b92d2a356f85c76a60a12ef6ea73fdbd81)
{: .prompt-tip }

Test 코드 수정버전 :
> [https://github.com/navilera/Navilos/commit/273b96189c6adf8f402f02f9f5ee103589492256](https://github.com/navilera/Navilos/commit/273b96189c6adf8f402f02f9f5ee103589492256)
{: .prompt-tip }

## 서론
***
컨텍스트 스위칭이란 이름 그대로 컨텍스트를 전환한다는 것입니다.

현재 태스크 컨텍스트를 태스크의 스택에 저장되어 있습니다.

그러므로 컨텍스트 스위칭은 아래와 같은 과정으로 진행됩니다.

1. 현재 동작하고 있는 태스크의 컨텍스트를 현재 스택에 백업
2. 다음에 동작할 태스크 컨트롤 블록을 스케줄러에서 받아오기
3. 2에서 받은 태스크 컨트롤 블럭에서 스택 포인터 읽기
4. 3에서 읽은 태스크의 스택에서 컨텍스트를 읽어서 ARM 코어에 복구
5. 다음에 동작할 태스크의 직전 프로그램 실행 위치로 이동.<br>
  (이러면 이제 현재 동작하고 있는 태스크가 됨)

이를 코드로 옮기면 아래와 같습니다.

### `kernel/task.c` 스케줄러 함수

```c
static KernelTcb_t* sCurrent_tcb;
static KernelTcb_t* sNext_tcb;
...
...
  ...
void Kernel_task_scheduler(void)
{
  sCurrent_tcb = &sTask_list[sCurrent_tcb_index];
  sNext_tcb = Scheduler_round_robin_algorithm();

  Kernel_task_context_switching();
}
```

`sCurrent_tcb` 는 현재 동작 중인 태스크 컨트롤 블록의 포인터입니다.

`sNext_tcb`는 라운드 로빈 알고리즘이 선택한 다음에 동작할 TCB의 포인터입니다.

이 두 포인터를 확보한 후, 11번째 줄에서 컨텍스트 스위칭 함수를 호출합니다.

### 컨텍스트 스위칭 함수

```c
__attribute__ ((naked)) void Kernel_task_context_switching(void)
{
	__asm__ ("B Save_context");
	__asm__ ("B Restore_context");
}
```

### `__attribute__ ((naked))`

: GCC 컴파일러의 어트리뷰트 기능인데, 어트리뷰트를 naked라고 설정하면 컴파일러가 함수를 컴파일할 때 자동으로 만드는 스택 백업, 복구, 리턴 관련 **어셈블리어가 전혀 생성되지 않고** 내부에 코딩한 코드 자체만 남습니다.

위 함수를 역어셈블해서 보면 아래와 같습니다.

```bash
0000021c <Kernel_task_context_switching>:
 21c:  ea000000    b    224 <Save_context>
 220:  ea000006    b    240 <Restore_context>
```

위처럼 인라인 어셈블리로 코딩한 두 줄이 그대로 컴파일되고 다른 코드는 없습니다.

어트리뷰트 기능을 사용하지 않으면 아래와 같은 역어셈블 결과가 나옵니다.

```bash
0000021c <Kernel_task_context_switching>:
 21c:  e52db004    push {fp}    ; (str fp, [sp, #-4]!)
 220:  528db000    add  fp, sp, #0
 224:  ea000000    b    238 <Save_context>
 228:  ea000006    b    254 <Restore_context>
 22c:  e24bd000    sub  sp, fp, #0
 230:  e49db004    pop  {fp}    ; (ldr fp. [sp], #4)
 234:  e12fff1e    bx   lr
```

> C 언어 코드 파일에서 코딩한 내용의 앞뒤로 스택을 확보하는 코드와 리턴하는 코드가 추가됨.
{: .prompt-info }

Navilos는 컨텍스트를 스택에 백업하고 스택에 복구할 것이므로 컨텍스트 스위칭할 때 되도록 스택을 그대로 유지하는 것이 좋습니다.

→ `__attribute__ ((naked))` 기능을 사용

![image.png](/assets/img/OS/os1001.png)

위 그림은 Navilos의 컨텍스트 스위칭 과정을 설명하는 그림입니다.

Task#1이 현재 동작 중인 태스크입니다. Task#2가 다음 동작할 태스크입니다.

Task#1의 현재 스택 포인터에 그대로 현재 컨텍스트를 백업합니다.

**✨스택 포인터만 따로 저장하는 이유**<br>
: 커널이 스택 포인터의 위치를 쉽게 가져올 수 있어야 스택에서 컨텍스트를 복구할 수 있다.

Task#2의 TCB에서 스택 포인터 값을 읽습니다. 그리고 범용 레지스터 SP에 그 값을 씁니다.

그러면 ARM 코어에서는 스택 포인터가 바로 바뀝니다.

그 상태에서 스택 관련 어셈블리 명령을 사용해서 컨텍스트를 복구합니다.

컨텍스트를 복구하면서 자연스럽게 스택 포인터를 Task#2가 컨텍스트 스위칭을 하기 직전의 정상적인 스택 포인터 위치로 복구합니다.

Navilos의 컨텍스트 스위칭은 엄밀하게 말하면 윈도우나 리룩스의 프로세스 전환하는 컨텍스트 스위칭보다는 한 프로세스 안에서 쓰레드(thread) 간에 전환을 하는 모습에 더 가깝습니다.


## 컨텍스트 백업하기
***
컨텍스트는 현재 동작 중인 태스크의 스택에 직접 백업하니다.

따라서 앞서 정의한 컨텍스트 자료 구조에 따라 스택 명령어의 순서를 맞춰야 합니다.

```c
typedef struct KernelTaskContext_t
{
	uint32_t spsr;
	uint32_t r0_r12 [13];
	uint32_t pc;
} KernelTaskContext_t;
```

`spsr`, `r0_r12` , `pc` 순서입니다.

스택은 메모리 주소가 큰 값에서 작은 값으로 진행하기에, 이 구조체에 맞춰 컨텍스트를 백업할 땐 `pc`, `r0_r12` , `spsr` 순서로 백업해야 의도한 자료 구조에 맞게 값이 저장됩니다.

### 컨텍스트 백업 코드

```c
static __attribute__ ((naked)) void Save_context(void)
{
  // save current task context into the current task stack
  __asm__ ("PUSH {lr}");
  __asm__ ("PUSH {r0, r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12}");
  __asm__ ("MRS   r0, cpsr");
  __asm__ ("PUSH {r0}");
  // save current task stack pointer into the current TCB
  __asm__ ("LDR   r0, =sCurrent_tcb");
  __asm__ ("LDR   r0, [r0]");
  __asm__ ("STMIA r0!, {sp}");
}
```

**4번째 줄**에서 LR을 스택에 푸시합니다. → `pc` 멤버 변수 저장

나중에 태스크가 다시 스케줄링을 받았을 때 복귀하는 위치는 `pc` 멤버 변수가 저장하고 있고, 이 위치는 `Kernel_task_context_switching()` 함수의 리턴 주소입니다.

그러므로 `pc` 멤버 변수에 현재 컨텍스트의 LR값을 그대로 저장하는 것입니다.

**5번째 줄**이 범용 레지스터인 R0부터 R12까지 스택에 푸시하는 코드입니다.

여기까지 진행하며 현재 R0부터 R12에 다른 값을 덮어 쓰지 않았으므로 이 값은 스위칭 함수를 호출하기 **전 값이 유지**되고 있습니다.

→ 어트리뷰트 ((naked)) 지시어를 사용하는 이유

**6,7번째 줄**이 CPSR을 `KernelTaskContext_t` 의 `spsr` 멤버 변수 위치에 저장하는 코드입니다.

프로그램 상태 레지스터는 직접 메모리에 저장할 수 없으니 R0를 사용합니다.

**9번째 줄**은 현재 동작 중인 TCB의 포인터 변수를 읽는 코드입니다.

**10번째 줄**에서 포인터에 저장된 값을 읽습니다.

포인터에 저장된 값이 주솟값이므로 `r0`로 TCB의 온전한 메모리 위치를 읽습니다.

**11번째 줄**은 10번째 줄에서 읽은 값을 베이스 메모리 주소로 해서 SP를 저장하는 코드입니다.

9~11번째 줄을 C언어로 표현하면

```c
sCurrent_tcb->sp = ARM_코어_SP_레지스터값;
// 혹은
(uint32_t)(*sCurrent_tcb) = ARM_코어_SP_레지스터값;
```

> TCB 구조체의 첫 번째 멤버 변수가 `sp` 이므로 포인터 값을 읽어 사용할 수 있습니다.
두 번째 멤버 변수일 경우 4바이트를 더해야 하나, 추가 작업을 없애기 위해 첫 번째 멤버 변수로 사용합니다.
{: .prompt-tip }

## 컨텍스트 복구하기
***
컨텍스트를 복구하는 작업은 백업하는 작업의 역순입니다.

### 컨텍스트 복구 코드

```c
static __attribute__ ((naked)) void Restore_context(void)
{
  // restore next task stack pointer from the next TCB
  __asm__ ("LDR   r0, =sNext_tcb");
  __asm__ ("LDR   r0, [r0]");
  __asm__ ("LDMIA r0!, {sp}");
  // restore next task context from the next task stack
  __asm__ ("POP  {r0}");
  __asm__ ("MSR   cpsr, r0");
  __asm__ ("POP  {r0, r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12}");
  __asm__ ("POP  {pc}");
}

```

형태가 `Save_context()` 함수와 동일하며, 동작만 역순일 뿐입니다.

- `sNext_tcb` 에서 스택 포인터 값 읽어오기

4~6번째 줄이 TCB의 `sp` 멤버 변수의 값을 읽어 ARM 코어의 SP에 쓰는 작업입니다.

- CSPR 복구

8,9번째 줄이 스택에 저장된 `cpsr` 값을 꺼내 ARM코어의 CPSR에 쓰는 작업입니다.

백업 시 PUSH 명령을 사용하고 복구할 땐 POP 명령을 사용합니다.

- 범용 레지스터 복구

10번째 줄이 R0 ~ R12 범용 레지스터를 복구하는 코드입니다.

이 시점 이후로는 R0 ~ R12 값을 변경하면 복구에 실패하게 됩니다.

그래서 11번째 줄에서 스택 값을 꺼내 PC에 저장하면서 태스크 코드로 점프합니다.

11번째 줄이 실행되는 순간 ARM코어는 컨텍스트 백업 전 코드 위치로 PC를 옮기고 실행을 이어서 합니다. 

## yield 만들기
***
스케줄러와 컨텍스트 스위칭이 있으면 태스크를 전환할 수 있습니다.

**스케줄러와 컨텍스트 스위칭을 합쳐 스케줄링(scheduling)**이라 합니다.

### 시분할 시스템

정기적으로 발생하는 타이머 인터럽트에 연동해 스케줄링하고

각 태스크가 일정한 시간만 동작하고 다음 태스크로 전환되는 시스템

ex) 100ms마다 스케줄링을 하도록 설정하면 태스크들은 각각 100ms씩 동작하고 다음 태스크로.

### 선점형 멀티태스킹 시스템

태스크가 명시적으로 스케줄링을 요청하지 않았는데 커널이 강제로 스케줄링하는 시스템

### 비선점형 멀티태스킹 시스템

반대로 태스크가 명시적으로 스케줄링을 요청하지 않으면 커널이 스케줄링하지 않는 시스템

> **일반적으로 시분할 시스템은 거의 선점형 멀티태스킹 시스템!**
{: .prompt-tip }

Navilos 프로젝트에선 시분할이 아닌 비선점형 스케줄링을 사용

→ 스케줄링하려면 태스크가 명시적으로 커널에 스케줄링을 요청해야 함

→ 태스크가 CPU 자원을 다음 태스크에 양보한다는 의미로 해석 가능

→ `yield()` 함수 작성해야 한다!

### `yield()` 함수 작성하기

커널 API를 별도로 만들어 외부에서 사용

`kernel/Kernel.c` 와 `kernel/Kernel.h` 파일 만들기

### `kernel/Kernel.h` yield 커널 API 정의

```c
#ifndef KERNEL_KERNEL_H_
#define KERNEL_KERNEL_H_

#include "task.h"

void Kernel_yield(void);

#endif /* KERNEL_KERNEL_H_ */
```

### `kernel/Kernel.c` yield 커널 API 구현

```c
#include "stdint.h"
#include "stdbool.h"

#include "Kernel.h"

void Kernel_yield(void)
{
	Kernel_task_scheduler();
}
```

구현은 매우 간단하게 `Kernel_task_scheduler()` 함수를 호출하면 됩니다. 

태스크가 더 이상 할 일이 없을 때 `Kernel_yield()` 함수를 호출

→ 즉시 스케줄러를 호출해 다음에 동작할 태스크를 선정합니다.

→ 컨텍스트 스위칭 수행

→ `Kernel_yield()` 함수를 호출한 태스크의 컨텍스트를 백업하고 스케줄러가 선정한 태스크의 스택 포인터를 복구

→ 스택 포인터로부터 컨텍스트 복구

→ 다음 동작할 코드의 위치는 태스크의 `Kernel_yield()`의 리턴 코드 직전

→ 즉, 스케줄링 직후로 돌아와서 다음 태스크가 CPU를 사용

## 커널 시작하기
***
앞 장에서 커널에 태스크를 세 개 생성했습니다(더미 태스크).

이제 스케줄러도 있고 컨텍스트 스위칭도 있으니 커널을 시작해 태스크 세 개를 동작시킬 수 있습니다.

하지만, 처음 커널을 시작할 때 스케줄러를 그냥 실행하면 **태스크가 동작하지 않습니다.**

→ 커널을 **시작할 땐 현재 동작 중인 태스크가 없기 때문**!

**즉, 최초로 스케줄러를 실행할 때는 컨텍스트 백업을 하지 않아야 합니다.**

최초 스케줄링 시 컨텍스트 복구만 하면 됩니다.

최초 스케줄링이니까 스케줄러를 거치지 말고 그냥 0번 TCB를 복구 대상으로 삼습니다.

커널 소스 코드에 있는 TCB 인덱스를 저장하고 있는 정적 전역 변수의 초기 값을 바로 사용합니다.

### `kernel/task.c` 첫 번째 스케줄링만 처리하는 코드

```c
static KernelTcb_t  sTask_list[MAX_TASK_NUM];
static KernelTcb_t* sCurrent_tcb;
static KernelTcb_t* sNext_tcb;
static uint32_t     sAllocated_tcb_index;
static uint32_t     sCurrent_tcb_index;
...
...
...
void Kernel_task_init(void)
{
    sAllocated_tcb_index = 0;
    sCurrent_tcb_index = 0;
    for(uint32_t i = 0 ; i < MAX_TASK_NUM ; i++)
    {
        sTask_list[i].stack_base = (uint8_t*)(TASK_STACK_START + (i * USR_TASK_STA    CK_SIZE));
        sTask_list[i].sp = (uint32_t)sTask_list[i].stack_base + USR_TASK_STACK_SIZ    E - 4;
        sTask_list[i].sp -= sizeof(KernelTaskContext_t);
        KernelTaskContext_t* ctx = (KernelTaskContext_t*)sTask_list[i].sp;
        ctx->pc = 0;
        ctx->spsr = ARM_MODE_BIT_SYS;
    }
}
void Kernel_task_start(void)
{
    sNext_tcb = &sTask_list[sCurrent_tcb_index];
    Restore_context();
}

```

5번째 줄은 현재 실행 중인 태스크의 TCB 인덱스를 저장하고 있는 정적 전역 변수를 선언합니다.

이 변수를 12번째 줄에서 0으로 초기화합니다.

그리고 26~30번째 줄은 커널이 시작할 때 최초 한 번만 호출하는 함수입니다.

**이전에 말했듯 0번 TCB를 다음 태스크로 선정하고, 컨텍스트 복구만 실행합니다.(백업없이)**

이제, `Kernel_task_start()` 함수를 커널 API인 `Kernel_start()` 함수에 연결하고 `main()`에서 호출해 실행해 보겠습니다.

### `kernel/Kernel.h` `Kernel_start()` 함수를 추가

```c
#ifndef KERNEL_KERNEL_H_
#define KERNEL_KERNEL_H_

#include "task.h"

**void Kernel_start(void);**
void Kernel_yield(void);

#endif /* KERNEL_KERNEL_H_ */
```

앞으로 추가할 커널 관련 초기화 함수를 `Kernel_start()` 함수에 모아서 한번에 실행할 계획입니다.

`Kernel_start()` 함수에서 커널 초기화를 담당하는 것입니다.

### `kernel/Kernel.c` 최초의 `Kernel_start()` 함수

```c
void Kernel_start(void)
{
	Kernel_task_start();
}
```

`Kernel_start()` 함수를 호출하는 코드를 `main()` 함수에 추가하고 QEMU를 실행해 보겠습니다.

### `boot/Main.c` `main()` 함수에서 커널 시작하기

```c
static void Kernel_init(void)
{
    uint32_t taskId;

    Kernel_task_init();

    taskId = Kernel_task_create(User_task0);

    if (NOT_ENOUGH_TASK_NUM == taskId)
    {
        putstr("Task0 creation fail\n");
    }

    taskId = Kernel_task_create(User_task1);

    if (NOT_ENOUGH_TASK_NUM == taskId)
    {
        putstr("Task1 creation fail\n");
    }

    taskId = Kernel_task_create(User_task2);

    if (NOT_ENOUGH_TASK_NUM == taskId)
    {
        putstr("Task2 creation fail\n");
    }

    Kernel_start();
}
```

여기에 더불어 사용자 새트크가 제대로 스택을 할당받았는지 확인해 보는 코드를 추가해 정상적으로 동작하는지 확인하겠습니다.

아무 이름으로나 로컬 변수를 하나 선언하고 `debug_printf()`로 변수의 주소 값을 출력합니다.

### 사용자 태스크의 스택 주소를 확인하는 코드

```c
void User_task0(void)
{
  uint32_t local = 0;

  while(true)
  {
    debug_printf("User Task #0 SP=0x%x\n", &local);
    Kernel_yield();
  }
}

void User_task1(void)
{
  uint32_t local = 0;

  while(true)
  {
    debug_printf("User Task #1 SP=0x%x\n", &local);
    Kernel_yield();
  }
}

void User_task2(void)
{
  uint32_t local = 0;

  while(true)
  {
    debug_printf("User Task #2 SP=0x%x\n", &local);
    Kernel_yield();
  }
}

```

세 태스크 모두 `local` 이름으로 로컬 변수를 선언하고 해당 변수의 주소 값을 출력하는 코드가 이어집니다.

의도한 대로 동작한다면 세 태스크의 로컬 변수 주소(스택 주소)가 출력될 것입니다.

```bash
$ qemu-system-arm -M realview-pb-a8 -kernel build/navilos.axf -nographic
```

![image.png](/assets/img/OS/os1002.png)

각 태스크의 스택 주소 차이가 0x10 0000이므로 간격이 딱 1MB씩입니다.

일단 스택 간격은 의도한대로 잘 할당되었습니다.

그리고, `include/MemoryMap.h` 파일에서 `TASK_STACK_START` 의 값을 0x80 0000으로 작성했습니다.

이 값이 Task#0의 스택 베이스 주소입니다.

스택 포인터에는 스택 공간의 최댓값을 할당합니다.

그리고 태스크 스택 간 4바이트 간격을 패딩으로 설계했습니다.

→ TCB 초기화 후 할당된 스택 포인터의 초기 값은 0x8F FFFC입니다.

여기에 컴파일러가 사용하는 스택이 몇 개 되고 그다음에 로컬 변수가 스택에 잡히므로

0x8F FFF0으로 출력된 것입니다.

## 요약
***
이 장에서는 컨텍스트 스위칭을 만들었습니다.

일반적으로 쉽게 이해할 수 있는 함수 호출-리턴이 아닌

강제로 컨텍스트를 백업-리스토어하는 것이라 어려웠을 것입니다.

다음 장에서는 이벤트를 구현해 일단 태스크 간 간단한 신호부터 주고받아 보겠습니다.

## 참고
***

참고 깃허브 : [https://github.com/navilera/Navilos](https://github.com/navilera/Navilos)

이만우 저자님의 블로그 주소 : [https://kldp.org/node/162560](https://kldp.org/node/162560)

