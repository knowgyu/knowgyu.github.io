---
title: "Misc. Pipeline 설명"
author: knowgyu
description: " "
date: 2023-12-10 09:43:26 +0900
math: true
categories: [MLOps, 4-Kubeflow-Pipeline-YOLOv8]
tags: [MLOps, Kubeflow]
---

본 문서에서는 지금까지 작성한 커스텀 데이터셋 학습 파이프라인에 대한 설명과 설정에 대해 다룹니다.

# Training Pipeline

---

## Pipeline Graph

---

![Untitled](/assets/img/kubeflow/kubeyolo501.png){: .w-50}

### Volume-mount

파이프라인에서 각 Pod들이 Node 로컬 볼륨에 **마운트**하기 위해 `pvc` 를 생성하는 Op입니다. 대부분의 작업이 데이터를 읽고 저장해야하기에, 필수적인 요소입니다. 

### Verify pipeline op

파이프라인 실행 시 **입력**한 파라미터들이 **유효**한 지 확인하는 Op입니다. 모델의 경로, 학습 CFG파일, 데이터셋 yaml파일을 입력받아 확장자가 없다면, 이를 추가하고 해당 파일들이 폴더에 존재하는 지 확인합니다.

### Get data distribution op

데이터의 **분포**를 파악하는 Op입니다. 입력받은 데이터셋 yaml파일을 읽어 데이터셋의 위치와 클래스 이름을 저장하고, 데이터셋의 라벨링 파일을 통해 클래스 별 이미지 파일의 수와 인스턴스 수를 구합니다. 이후 Plot을 위해 데이터 분포를 반환하고, Pipeline UI를 통해 데이터 분포표를 출력합니다.

### Plot data distribution op

데이터의 분포를 `matplotlib`를 이용해 시각화합니다. 

클래스별 이미지 파일의 수와 인스턴스 수를 Plot합니다.

### Train op

모델을 학습합니다. 학습 설정 변경 및 하이퍼파라미터 조정은 cfg파일을 이용합니다. 또한, MLflow에 로깅합니다.

### Tune op

학습 후 `best.pt`를 불러와 하이퍼 파라미터 튜닝을 진행합니다. 또한, MLflow에 로깅합니다.

### Test op

학습 후 `best.pt`를 불러와 테스트 데이터셋에 대한 모델의 일반화 성능을 평가합니다. 또한, MLflow에 로깅합니다.



## Pipeline Config

---

파이프라인 실행 시 필요한 파라미터들은 아래와 같습니다.

![Kubeflow Dashboard → Runs → Create Run → Choose Pipeline](/assets/img/kubeflow/kubeyolo502.png)

Kubeflow Dashboard → Runs → Create Run → Choose Pipeline

- `model_name`(str) : 학습할 모델의 파일명을 입력합니다. (`.pt` 혹은 `.yaml`)
- `cfg`(str) : YOLOv8 학습 구성 및 하이퍼파라미터를 담고있는 cfg파일의 파일명을 입력합니다.
- `data`(str) : 학습할 데이터셋의 정보를 담고있는 yaml파일의 파일명을 입력합니다.
- `bool_train`(bool) : True 입력 시 모델을 학습합니다.
- `bool_tune`(bool) : True 입력 시 모델 학습 후 `best.pt`모델의 하이퍼 파라미터를 튜닝합니다.

<br>
💡 `bool_train` , `bool_tune` 이 각각 False, True일 경우에 Tuning을 진행하지 않습니다.


## Pipeline Settings

---

파이프라인을 정의하는 함수는 아래와 같습니다.

```python
@dsl.pipeline(name="pipelinename",
          description="MLpipline Description",
          )
def train_pipeline(
    model_name: str = 'yolov8n.pt',
    cfg: str = 'cfg-custom',
    data: str = 'data-custom',
    bool_train: bool=True,
    bool_tune: bool=False,
    ):
    
    vop = dsl.VolumeOp(
        name="volume mount",
        resource_name="pvc-local-cocodata",
        storage_class='train',
        modes=dsl.VOLUME_MODE_RWO,
        size="3Gi",
        generate_unique_name=False,
        action='apply',
        )
    
    
    
    verifier = verify_pipeline_op(model_name, cfg, data)
    
    get_distribution = get_data_distribution_op(verifier.outputs['data_yaml'])
    
    plotting = plot_data_distribution_op(get_distribution.outputs["result_json"], get_distribution.outputs["cls_names_json"])
    
    with dsl.Condition(bool_train == True , "train"):
        trainer = train_op(verifier.outputs['model_path'], verifier.outputs['cfg_yaml'], verifier.outputs['data_yaml'])
        tester = test_op(trainer.outputs['last_pt'], trainer.outputs['best_pt'], trainer.outputs['data_yaml'])
        
        with dsl.Condition(bool_tune == True , 'tune'):
            raytuner = tune_op(trainer.outputs['last_pt'],trainer.outputs['best_pt'],trainer.outputs['data_yaml'])        

    # Volume Mount
    verifier.add_pvolumes({"/data":vop.volume})
    get_distribution.add_pvolumes({"/data":verifier.pvolume})
    plotting.add_pvolumes({"/data":get_distribution.pvolume})
    trainer.add_pvolumes({"/data":plotting.pvolume})
    tester.add_pvolumes({"/data":trainer.pvolume})
    raytuner.add_pvolumes({"/data":trainer.pvolume})

    # shm-memory Resize(using EmptyDir)
    shm_volume = V1Volume(name="dshm", 
                          empty_dir = V1EmptyDirVolumeSource(medium='Memory', size_limit='1Gi'))
    shm_pvolume = dsl.PipelineVolume(volume=shm_volume)
    # Resize shm size
    verifier.add_pvolumes({"/dev/shm":shm_pvolume})
    get_distribution.add_pvolumes({"/dev/shm":shm_pvolume})
    plotting.add_pvolumes({"/dev/shm":shm_pvolume})
    trainer.add_pvolumes({"/dev/shm":shm_pvolume})
    tester.add_pvolumes({"/dev/shm":shm_pvolume})
    raytuner.add_pvolumes({"/dev/shm":shm_pvolume})
    
    # Disable caching. Two Options 
    '''
    This Option doesn't work well
    Use `.execution_options.caching_strategy.max_cache_staleness = "P0D"` instead.
    
    # verifier.set_caching_options(False)
    # get_distribution.set_caching_options(False)
    # plotting.set_caching_options(False)
    # trainer.set_caching_options(False)
    # raytuner.set_caching_options(False)
    '''
    verifier.execution_options.caching_strategy.max_cache_staleness = "P0D"
    get_distribution.execution_options.caching_strategy.max_cache_staleness = "P0D"
    plotting.execution_options.caching_strategy.max_cache_staleness = "P0D"
    trainer.execution_options.caching_strategy.max_cache_staleness = "P0D"
    tester.execution_options.caching_strategy.max_cache_staleness = "P0D"
    raytuner.execution_options.caching_strategy.max_cache_staleness = "P0D"
    
    # UI Component name settings (Default is Op name)
    '''
    vop.set_display_name("UI Component name settings")
    verifier.set_display_name("UI Component name settings")
    get_distribution.set_display_name("UI Component name settings")
    plotting.set_display_name("UI Component name settings")
    trainer.set_display_name("UI Component name settings")
    tester.set_display_name("UI Component name settings")
    raytuner.set_display_name("UI Component name settings")
    '''

if __name__ == "__main__":
    kfp.compiler.Compiler().compile(train_pipeline, "knowgyu_almost.yaml")
```

- `@dsl.pipeline` : `kfp` 라이브러리를 이용해 파이프라인을 생성합니다.
    - `name` : 파이프라인의 이름. 기본값은 함수명을 사용합니다.
    - `description` : (Optional) 파이프라인 설명
    - `pipeline_root` : (Optional) Input/Output URI placeholder가 사용되는 경우 필요합니다.

- `def train_pipeline(*args)` : 파이프라인을 정의합니다. args를 통해 파이프라인의 Config를 정의합니다.
- `vop = dsl.VolumeOp()` : PVC를 생성합니다. 
💡본 문서에서는 기존에 생성한 PVC를 사용하기에, `generate_unique_name=False` 옵션을 사용하였습니다.

### 컴포넌트 실행

```python
verifier = verify_pipeline_op(model_name, cfg, data)

get_distribution = get_data_distribution_op(verifier.outputs['data_yaml'])

plotting = plot_data_distribution_op(get_distribution.outputs["result_json"], get_distribution.outputs["cls_names_json"])

with dsl.Condition(bool_train == True , "train"):
    trainer = train_op(verifier.outputs['model_path'], verifier.outputs['cfg_yaml'], verifier.outputs['data_yaml'])
    tester = test_op(trainer.outputs['last_pt'], trainer.outputs['best_pt'], trainer.outputs['data_yaml'])
    
    with dsl.Condition(bool_tune == True , 'tune'):
        raytuner = tune_op(trainer.outputs['last_pt'],trainer.outputs['best_pt'],trainer.outputs['data_yaml'])        
```

실행할 컴포넌트를 호출합니다. Python 함수 호출 방식과 동일합니다.

단, 조건문 사용 시 `kfp.dsl.Condition`을 사용해야합니다.

### 볼륨 마운트 (+ 컴포넌트 실행 순서)

```python
# Volume Mount
verifier.add_pvolumes({"/data":vop.volume})
get_distribution.add_pvolumes({"/data":verifier.pvolume})
plotting.add_pvolumes({"/data":get_distribution.pvolume})
trainer.add_pvolumes({"/data":plotting.pvolume})
tester.add_pvolumes({"/data":trainer.pvolume})
raytuner.add_pvolumes({"/data":trainer.pvolume})
```

> Updates the existing pvolumes dict, extends volumes and volume_mounts and redefines the pvolume attribute.
> 

볼륨을 마운트합니다. pvolume dict를 입력할 때, 모두 `vop.volume`을 입력해도 되지만, 위처럼 마운트할 경우 순서를 지정할 수 있습니다.

혹은, 직접 `after()`를 이용해 컴포넌트 실행 순서를 지정할 수 있습니다.

```python
# Volume Mount
verifier.add_pvolumes({"/data":vop.volume})
get_distribution.add_pvolumes({"/data":vop.volume})
plotting.add_pvolumes({"/data":vop.volume})
trainer.add_pvolumes({"/data":vop.volume})
tester.add_pvolumes({"/data":vop.volume})
raytuner.add_pvolumes({"/data":vop.volume})

# after()를 이용해 순서 정하기
get_distribution.after(verifier)
plotting.after(get_distribution)
trainer.after(plotting)
tester.after(trainer)
raytuner.after(trainer)
```

> 컴포넌트 실행 순서를 **지정하지 않을 경우** ContainerOp가 **순서 없이 병렬적으로 실행**될 수 있습니다!
>
>위 컴포넌트 실행의 Argument들을 확인해보면, `trainer`컴포넌트는 모두 `verifier` 컴포넌트의 결과물만을 인자로 받습니다.
>만약, 순서를 지정하지 않을 경우, 데이터 분포를 분석하고 plot하는 컴포넌트와 학습하고 테스트하는 컴포넌트가 병렬적으로 실행되게니다.
{: .prompt-info }


### Share Memory(shm) 크기 변경

쿠버네티스에서 Pod가 생성되면 기본적으로 64MB의 shm 공유 메모리가 할당됩니다.

하지만, 이 공간만으로는 학습과 같은 작업 시 공유메모리가 부족해 에러가 발생하는 상황이 종종 발생하게됩니다.

> ~~주로 데이터 로드 시 멀티프로세싱 방식으로 로드할 때 공유 메모리 부족에러가 발생했습니다.~~

Docker에서는 이러한 상황이 발생했을 때 `--ipc=host` 혹은 `--shm-size=256m` 과 같은 옵션을 사용해 해결할 수 있습니다.

하지만, 쿠버네티스 환경에서 `ipc=host` 옵션 사용 시 보안 이슈가 발생할 수 있으며, shm 사이즈를 조정할 수 있는 별도의 옵션을 제공하지 않습니다.

이러한 문제를 쿠버네티스의 memory타입 EmptyDir 마운트 방식으로 문제를 해결할 수 있습니다. 

[⚠️ Shared Memory(shm) Error](https://knowgyu.github.io/posts/Yolov8-%ED%95%99%EC%8A%B5pp%EC%8B%A4%ED%96%89/#%EF%B8%8F-shared-memoryshm-error)

> [https://ykarma1996.tistory.com/106](https://ykarma1996.tistory.com/106)

위 블로그를 통해 yaml파일을 수정하여 shm 사이즈를 조정할 수 있습니다.

하지만, 이 방법을 사용할 경우 아래와 같이 작업이 번거로워집니다.

Python 코드 → yaml로 컴파일 → yaml파일에서 각 Pod를 찾아 마운트(현재 6개 Pod)

따라서, 파이썬으로 파이프라인을 작성할 때 볼륨을 마운트 할 수 있도록 하겠습니다.

```python
@dsl.pipeline(name="pipelinename",
          description="MLpipline Description",
          )
def train_pipeline(
...생략
    ):
    
...생략

    # shm-memory Resize(using EmptyDir)
    shm_volume = V1Volume(name="dshm", 
                          empty_dir = V1EmptyDirVolumeSource(medium='Memory', size_limit='1Gi'))
    shm_pvolume = dsl.PipelineVolume(volume=shm_volume)

    # shm Mount
    verifier.add_pvolumes({"/dev/shm":shm_pvolume})
    get_distribution.add_pvolumes({"/dev/shm":shm_pvolume})
    plotting.add_pvolumes({"/dev/shm":shm_pvolume})
    trainer.add_pvolumes({"/dev/shm":shm_pvolume})
    tester.add_pvolumes({"/dev/shm":shm_pvolume})
    raytuner.add_pvolumes({"/dev/shm":shm_pvolume})
```

- `from kubernetes.client import V1Volume, V1EmptyDirVolumeSource`
    
    V1Volume : Pod의 스토리지 볼륨을 추상화합니다. 컨테이너는 이 객체를 참조해 스토리지 볼륨을 마운트할 위치를 결정합니다.(PVC같은 역할)
    
    V1EmptyDirVolumeSource : Pod가 실행되는 동안 유지되는 임시 볼륨입니다. Pod가 종료되거나 재시작되면 데이터는 삭제됩니다.
    

### 캐싱기능 종료

```python
# Disable caching. Two Options 
'''
This Option doesn't work well
Use `.execution_options.caching_strategy.max_cache_staleness = "P0D"` instead.

# verifier.set_caching_options(False)
# get_distribution.set_caching_options(False)
# plotting.set_caching_options(False)
# trainer.set_caching_options(False)
# raytuner.set_caching_options(False)
'''
verifier.execution_options.caching_strategy.max_cache_staleness = "P0D"
get_distribution.execution_options.caching_strategy.max_cache_staleness = "P0D"
plotting.execution_options.caching_strategy.max_cache_staleness = "P0D"
trainer.execution_options.caching_strategy.max_cache_staleness = "P0D"
tester.execution_options.caching_strategy.max_cache_staleness = "P0D"
raytuner.execution_options.caching_strategy.max_cache_staleness = "P0D"
```

`set_caching_options(False)` 를 통해 기능을 종료할 수 있습니다. 혹은

`execution_options.caching_strategy.max_cache_staleness = "P0D"` 를 통해 종료할 수 있습니다.

첫번째 옵션의 경우 `False` 옵션으로 실행했음에도 불구하고, 캐시된 아웃풋을 이용해 컴포넌트 실행을 건너뛰는 경우가 발생했습니다.

따라서, Kubeflow 공식문서에서 기술한 두 번째 방법을 이용해 캐시 기능을 종료하는 것을 권장합니다.

> 첫번째 옵션의 경우, 캐시기능을 완전 종료하는게 아닌, 새롭게 캐시하지 않는 것을 의미하는 것 같습니다.
> 따라서, 만약 `.cache` 폴더를 삭제하고 첫번째 옵션 사용 시 정상적으로 작동할 수 있으나, 따로 테스트하진 않았습니다.
{: .prompt-tip }

### 컴포넌트 UI 이름 설정

```python
# UI Component name settings (Default is Op name)
'''
vop.set_display_name("UI Component name settings")
verifier.set_display_name("UI Component name settings")
get_distribution.set_display_name("UI Component name settings")
plotting.set_display_name("UI Component name settings")
trainer.set_display_name("UI Component name settings")
tester.set_display_name("UI Component name settings")
raytuner.set_display_name("UI Component name settings")
'''
```

Kubeflow Pipeline UI에 표시되는 이름을 설정할 수 있습니다. 기본값은 함수명으로 지정되며, 필요에 따라 직접 설정할 수 있습니다.

본 문서의 파이프라인은 중복되는 컴포넌트를 사용하지 않지만, 만약 동일한 컴포넌트를 여러 개 사용할 경우 어떤 컴포넌트가 어떤 역할을 하는 지 헷갈릴 수 있습니다. 이러한 경우 사용하면 휴먼 에러를 줄일 수 있습니다.

### 파이프라인 컴파일

```python
if __name__ == "__main__":
    kfp.compiler.Compiler().compile(train_pipeline, "knowgyu_almost.yaml")
```

파이썬 코드를 Kubeflow Pipeline에 업로드할 수 있도록 YAML파일로 컴파일합니다.

# YOLOv8 Training Configuration

---

위 파이프라인을 이용해 YOLOv8을 학습할 때 데이터셋 변경 혹은 학습 구성을 변경하는 것은 `yaml`파일을 통해 변경 가능합니다.

> [https://docs.ultralytics.com/usage/cfg/#tasks](https://docs.ultralytics.com/usage/cfg/#tasks)

![Kubeflow Dashboard → Runs → Create Run → Choose Pipeline](/assets/img/kubeflow/kubeyolo503.png)

Kubeflow Dashboard → Runs → Create Run → Choose Pipeline

위 예시와 같이 `custom`이라는 구성파일을 만들어 수정해 사용하거나, 혹은 프로젝트별로 `cfg-insight3.yaml` `data-insight3.yaml` 과 같이 생성해 사용할 수 있습니다.

위 구성파일들을 수정하는 방법은 Kubeflow Jupyter Web App을 사용하거나, 직접 로컬에서 구성파일을 수정할 수 있습니다.

## Edit Configuration file

![Untitled](/assets/img/kubeflow/kubeyolo504.png)

Kubeflow 대시보드에서 좌측 탭 중 Notebooks 클릭하면 위 이미지와 같이 사용중인 노트북들을 확인할 수 있습니다.

이 중, 학습 하려는 폴더에 마운트된 주피터 노트북을 확인한 후 `CONNECT` 를 클릭해 주피터 노트북에 접속합니다.

- `cfg-custom.yaml`

![Untitled](/assets/img/kubeflow/kubeyolo505.png)

- `data-custom.yaml`

![Untitled](/assets/img/kubeflow/kubeyolo506.png)

위와 같이 Jupyter Notebook에 접속해 마운트된 폴더에 있는 구성 파일들을 통해 데이터셋 변경 혹은 학습 설정을 변경할 수 있습니다.

또한, 파일이 늘어날 경우 `cfg-yaml`, `data-yaml`, `weights` 폴더를 생성해 각각의 폴더에 파일들을 정리하여 사용할 수 있습니다.

> 위와 같이 구성파일을 수정해 데이터셋을 변경하거나 학습 설정을 변경하는 것은 파이프라인 수정이 필요하지 않기에 바로 적용됩니다.
{: .prompt-tip }

### (23.12.14) Model from scratch

학습 구성, 데이터셋 구성 파일들과 같은 위치에 `yolov8-scratch.yaml` 을 둡니다.

이 파일을 이용해 상황에 따라 백본 혹은 헤드를 수정하여 모델을 생성할 수 있습니다.

- `yolov8-scratch.yaml`

```bash
# Ultralytics YOLO 🚀, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect

# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs

# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9

# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12

  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)

  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)

  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)

  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

> Pipeline 실행 시 `yolov8n-scratch.yaml` 혹은 `yolov8s-scratch.yaml` 과 같은 형식으로 입력해야합니다.
{: .prompt-info }

# 참고 사항

---

## Kubeflow Docs

[https://www.kubeflow.org/docs/components/pipelines/v1/introduction/](https://www.kubeflow.org/docs/components/pipelines/v1/introduction/)

본 문서에선 Pipelines SDK (v2)를 지원하지 않습니다. 따라서, Kubeflow Pipelines v1만 참고합니다.

## `kfp` 라이브러리(v1.8.22)

[https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html)

> Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning workflows based on Docker containers within the [Kubeflow](https://www.kubeflow.org/) project.
> 
> 
> Use Kubeflow Pipelines to compose a multi-step workflow ([pipeline](https://www.kubeflow.org/docs/components/pipelines/concepts/pipeline/)) as a [graph](https://www.kubeflow.org/docs/components/pipelines/concepts/graph/) of containerized [tasks](https://www.kubeflow.org/docs/components/pipelines/concepts/step/) using Python code and/or YAML. Then, [run](https://www.kubeflow.org/docs/components/pipelines/concepts/run/) your pipeline with specified pipeline arguments, rerun your pipeline with new arguments or data, [schedule](https://www.kubeflow.org/docs/components/pipelines/concepts/run-trigger/) your pipeline to run on a recurring basis, organize your runs into [experiments](https://www.kubeflow.org/docs/components/pipelines/concepts/experiment/), save machine learning artifacts to compliant [artifact registries](https://www.kubeflow.org/docs/components/pipelines/concepts/metadata/), and visualize it all through the [Kubeflow Dashboard](https://www.kubeflow.org/docs/components/central-dash/overview/).
> 

[https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html)

> 파이썬을 이용해 Kubeflow Pipeline을 작성할 때 사용되는 라이브러리로, 매우 자주 사용하고 확인합니다.
**v1.8.22 버전**

## 모두를 위한 MLOps

[https://mlops-for-all.github.io/docs/introduction/intro/](https://mlops-for-all.github.io/docs/introduction/intro/)

위 튜토리얼을 통해 클러스터를 구축하고, 파이프라인을 작성했습니다.

💡 진행 중 에러 발생 시 깃허브 이슈 참고
[https://github.com/mlops-for-all/mlops-for-all.github.io/issues](https://github.com/mlops-for-all/mlops-for-all.github.io/issues)

## 기타 블로그

마켓컬리 : “**Kurly만의 MLOps 구축하기 - 쿠브플로우 도입기**” [https://helloworld.kurly.com/blog/second-mlops/](https://helloworld.kurly.com/blog/second-mlops/)

당근마켓 : “**Kubeflow 파이프라인 운용하기**” [https://medium.com/daangn/kubeflow-파이프라인-운용하기-6c6d7bc98c30](https://medium.com/daangn/kubeflow-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EC%9A%B4%EC%9A%A9%ED%95%98%EA%B8%B0-6c6d7bc98c30)

삼성SDS : “**쿠버네티스 기반의 AI 플랫폼: 쿠브플로우(Kubeflow)**” [https://www.samsungsds.com/kr/insights/kubeflow.html](https://www.samsungsds.com/kr/insights/kubeflow.html)


---
이상으로 Kubeflow 설명을 마칩니다.
